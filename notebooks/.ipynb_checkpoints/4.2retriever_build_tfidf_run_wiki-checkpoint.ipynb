{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:16:02.120684Z",
     "start_time": "2019-03-11T22:16:01.125667Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "\n",
    "from multiprocessing import Pool as ProcessPool\n",
    "from multiprocessing.util import Finalize\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "\n",
    "from drqa import retriever\n",
    "from drqa import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:16:24.360536Z",
     "start_time": "2019-03-11T22:16:24.355195Z"
    }
   },
   "outputs": [],
   "source": [
    "DOC2IDX = None\n",
    "PROCESS_TOK = None\n",
    "PROCESS_DB = None\n",
    "\n",
    "def init(tokenizer_class, db_class, db_opts):\n",
    "    global PROCESS_TOK, PROCESS_DB\n",
    "    PROCESS_TOK = tokenizer_class()\n",
    "    Finalize(PROCESS_TOK, PROCESS_TOK.shutdown, exitpriority=100)\n",
    "    PROCESS_DB = db_class(**db_opts)\n",
    "    Finalize(PROCESS_DB, PROCESS_DB.close, exitpriority=100)\n",
    "\n",
    "\n",
    "def fetch_text(doc_id):\n",
    "    global PROCESS_DB\n",
    "    return PROCESS_DB.get_doc_text(doc_id)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    global PROCESS_TOK\n",
    "    return PROCESS_TOK.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:18:20.017763Z",
     "start_time": "2019-03-11T22:18:14.181596Z"
    }
   },
   "outputs": [],
   "source": [
    "db_class = retriever.get_class('sqlite')\n",
    "doc_db = db_class(\"/Users/zhengjiexu/Desktop/DrQA/data/wikipedia/docs.db\")\n",
    "doc_ids = doc_db.get_doc_ids()  \n",
    "DOC2IDX = {doc_id: i for i, doc_id in enumerate(doc_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(name):\n",
    "    if name == 'spacy':\n",
    "        return SpacyTokenizer\n",
    "    if name == 'corenlp':\n",
    "        return CoreNLPTokenizer\n",
    "    if name == 'regexp':\n",
    "        return RegexpTokenizer\n",
    "    if name == 'simple':\n",
    "        return SimpleTokenizer\n",
    "\n",
    "    raise RuntimeError('Invalid tokenizer: %s' % name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:18:44.560794Z",
     "start_time": "2019-03-11T22:18:44.557182Z"
    }
   },
   "outputs": [],
   "source": [
    "tok_class = tokenizers.get_class('simple') \n",
    "PROCESS_TOK = tok_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:18:53.425223Z",
     "start_time": "2019-03-11T22:18:53.422195Z"
    }
   },
   "outputs": [],
   "source": [
    "PROCESS_DB = doc_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:25:44.745111Z",
     "start_time": "2019-03-11T22:25:44.742258Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import murmurhash3_32\n",
    "def hash(token, num_buckets):\n",
    "    \"\"\"Unsigned 32 bit murmurhash for feature hashing.\"\"\"\n",
    "    return murmurhash3_32(token, positive=True) % num_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:24:49.129897Z",
     "start_time": "2019-03-11T22:24:49.123739Z"
    }
   },
   "outputs": [],
   "source": [
    "def count(ngram, hash_size, doc_id):\n",
    "    \"\"\"Fetch the text of a document and compute hashed ngrams counts.\"\"\"\n",
    "    global DOC2IDX\n",
    "    row, col, data = [], [], []\n",
    "    # Tokenize\n",
    "    tokens = tokenize(retriever.utils.normalize(fetch_text(doc_id)))\n",
    "\n",
    "    # Get ngrams from tokens, with stopword/punctuation filtering.\n",
    "    ngrams = tokens.ngrams(\n",
    "        n=ngram, uncased=True, filter_fn=retriever.utils.filter_ngram\n",
    "    )\n",
    "    print('----ngrams----')\n",
    "    print(ngrams[:10])\n",
    "    print('\\n----number of grams before hash----')\n",
    "    print(len(ngrams))\n",
    "\n",
    "    # Hash ngrams and count occurences\n",
    "    counts = Counter([retriever.utils.hash(gram, hash_size) for gram in ngrams])\n",
    "    print('\\n----number of grams after hash----')\n",
    "    print(len(counts))\n",
    "    \n",
    "\n",
    "    # Return in sparse matrix data format.\n",
    "    row.extend(counts.keys())\n",
    "    col.extend([DOC2IDX[doc_id]] * len(counts))\n",
    "    data.extend(counts.values())\n",
    "    return row, col, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:24:55.271070Z",
     "start_time": "2019-03-11T22:24:55.165931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ngrams----\n",
      "['black', 'black forest', 'forest', 'black', 'black forest', 'forest', 'large', 'large forested', 'forested', 'forested mountain']\n",
      "\n",
      "----number of grams before hash----\n",
      "6058\n",
      "\n",
      "----number of grams after hash----\n",
      "3198\n"
     ]
    }
   ],
   "source": [
    "row, col, data = count(ngram=2, hash_size=2**24, doc_id='Black Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:25:55.479907Z",
     "start_time": "2019-03-11T22:25:55.475912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13459119"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash('black forest', 2**24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:26:07.833558Z",
     "start_time": "2019-03-11T22:26:07.829048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13459119 in row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:26:34.417174Z",
     "start_time": "2019-03-11T22:26:34.413085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16396370, 13459119, 15897870, 12058803, 4973702]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row is hashed ngrams\n",
    "row[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:26:56.940377Z",
     "start_time": "2019-03-11T22:26:56.936287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[736352, 736352, 736352, 736352, 736352]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# col is doc index\n",
    "col[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:28:46.518277Z",
     "start_time": "2019-03-11T22:28:46.514068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[170, 168, 186, 11, 1]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data is count of hashed ngrams\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
