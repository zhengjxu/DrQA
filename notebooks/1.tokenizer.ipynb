{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:02:45.173015Z",
     "start_time": "2019-03-11T17:02:44.575083Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:16:09.622125Z",
     "start_time": "2019-03-11T17:16:07.832600Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:16:10.962736Z",
     "start_time": "2019-03-11T17:16:10.959925Z"
    }
   },
   "outputs": [],
   "source": [
    "text = 'I live in US\\n.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:16:11.211370Z",
     "start_time": "2019-03-11T17:16:11.206707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I live in US .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = text.replace('\\n', ' ')\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:16:11.482150Z",
     "start_time": "2019-03-11T17:16:11.477390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I live in US ."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nlp.tokenizer(clean_text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:16:29.407592Z",
     "start_time": "2019-03-11T17:16:29.399482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I live in US ."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tagger(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:16:29.790412Z",
     "start_time": "2019-03-11T17:16:29.781516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I live in US ."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.entity(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:16:49.767113Z",
     "start_time": "2019-03-11T17:16:49.762063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', '-PRON-', 'PRP', 0],\n",
       " ['live', 'live', 'VBP', 0],\n",
       " ['in', 'in', 'IN', 0],\n",
       " ['US', 'us', 'NNP', 382],\n",
       " ['.', '.', '.', 0]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[i.text, i.lemma_, i.tag_, i.ent_type] for i in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:13:29.489831Z",
     "start_time": "2019-03-11T17:13:29.485921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I live in US .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:13:22.064845Z",
     "start_time": "2019-03-11T17:13:22.060936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "7\n",
      "10\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokens)):\n",
    "    print(tokens[i].idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:14:54.664974Z",
     "start_time": "2019-03-11T17:14:54.660667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I ', 'live ')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:2], text[2:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:15:46.201617Z",
     "start_time": "2019-03-11T17:15:46.196847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0].idx, tokens[0].idx+len(tokens[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:20:28.937248Z",
     "start_time": "2019-03-11T17:20:28.930019Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(self, text):\n",
    "    # We don't treat new lines as tokens.\n",
    "    clean_text = text.replace('\\n', ' ')\n",
    "    tokens = self.nlp.tokenizer(clean_text)\n",
    "    if any([p in self.annotators for p in ['lemma', 'pos', 'ner']]):\n",
    "        self.nlp.tagger(tokens)\n",
    "    if 'ner' in self.annotators:\n",
    "        self.nlp.entity(tokens)\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(tokens)):\n",
    "        # Get whitespace\n",
    "        start_ws = tokens[i].idx  # start position\n",
    "        if i + 1 < len(tokens):\n",
    "            end_ws = tokens[i + 1].idx  # end position with whitespace\n",
    "        else:\n",
    "            end_ws = tokens[i].idx + len(tokens[i].text)\n",
    "\n",
    "        data.append((\n",
    "            tokens[i].text,\n",
    "            text[start_ws: end_ws],  # with whitespace\n",
    "            (tokens[i].idx, tokens[i].idx + len(tokens[i].text)),  # no whitespace\n",
    "            tokens[i].tag_,  # detailed POS\n",
    "            tokens[i].lemma_,\n",
    "            tokens[i].ent_type_,  # NER\n",
    "        ))\n",
    "\n",
    "    # Set special option for non-entity tag: '' vs 'O' in spaCy\n",
    "    return Tokens(data, self.annotators, opts={'non_ent': ''})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:22:45.983017Z",
     "start_time": "2019-03-11T17:22:45.980283Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:22:47.166944Z",
     "start_time": "2019-03-11T17:22:46.733049Z"
    }
   },
   "outputs": [],
   "source": [
    "from drqa.tokenizers import SpacyTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:22:58.408263Z",
     "start_time": "2019-03-11T17:22:56.488049Z"
    }
   },
   "outputs": [],
   "source": [
    "tok = SpacyTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:23:05.256781Z",
     "start_time": "2019-03-11T17:23:05.251987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<drqa.tokenizers.tokenizer.Tokens at 0x121ed9198>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.tokenize('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T17:23:11.006018Z",
     "start_time": "2019-03-11T17:23:11.001127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words methods is added in Tokens(object)\n",
    "tok.tokenize('hello world').words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = 0\n",
    "TEXT_WS = 1\n",
    "SPAN = 2\n",
    "POS = 3\n",
    "LEMMA = 4\n",
    "NER = 5\n",
    "    \n",
    "def words(self, uncased=False):\n",
    "    \"\"\"Returns a list of the text of each token\n",
    "\n",
    "    Args:\n",
    "        uncased: lower cases text\n",
    "    \"\"\"\n",
    "    if uncased:\n",
    "        return [t[self.TEXT].lower() for t in self.data]\n",
    "    else:\n",
    "        return [t[self.TEXT] for t in self.data]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
