{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool as ProcessPool\n",
    "from multiprocessing.util import Finalize\n",
    "\n",
    "def init(tokenizer_class, db_class, db_opts):\n",
    "    global PROCESS_TOK, PROCESS_DB\n",
    "    PROCESS_TOK = tokenizer_class()  # initialize tokenizer\n",
    "    Finalize(PROCESS_TOK, PROCESS_TOK.shutdown, exitpriority=100)  # used to exit tokenize process\n",
    "    PROCESS_DB = db_class(**db_opts)  # connect to DB\n",
    "    Finalize(PROCESS_DB, PROCESS_DB.close, exitpriority=100)   # use to exit process: close connection to DB\n",
    "\n",
    "# 4 tokenizers to choose\n",
    "tok_class = tokenizers.get_class(args.tokenizer)\n",
    "\n",
    "workers = ProcessPool(\n",
    "    args.num_workers,\n",
    "    initializer=init,\n",
    "    initargs=(tok_class, db_class, db_opts)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def count(ngram, hash_size, doc_id):\n",
    "    \"\"\"Fetch the text of a document and compute hashed ngrams counts.\"\"\"\n",
    "    global DOC2IDX\n",
    "    row, col, data = [], [], []\n",
    "    # Tokenize\n",
    "    tokens = tokenize(retriever.utils.normalize(fetch_text(doc_id)))\n",
    "\n",
    "    # Get ngrams from tokens, with stopword/punctuation filtering.\n",
    "    ngrams = tokens.ngrams(\n",
    "        n=ngram, uncased=True, filter_fn=retriever.utils.filter_ngram\n",
    "    )\n",
    "    print('----ngrams----')\n",
    "    print(ngrams[:10])\n",
    "    print('\\n----number of grams before hash----')\n",
    "    print(len(ngrams))\n",
    "\n",
    "    # Hash ngrams and count occurences\n",
    "    counts = Counter([retriever.utils.hash(gram, hash_size) for gram in ngrams])\n",
    "    print('\\n----number of grams after hash----')\n",
    "    print(len(counts))\n",
    "    \n",
    "\n",
    "    # Return in sparse matrix data format.\n",
    "    row.extend(counts.keys())\n",
    "    col.extend([DOC2IDX[doc_id]] * len(counts))\n",
    "    data.extend(counts.values())\n",
    "    return row, col, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_matrix(args, db, db_opts):\n",
    "    \"\"\"Form a sparse word to document count matrix (inverted index).\n",
    "\n",
    "    M[i, j] = # times word i appears in document j.\n",
    "    \n",
    "    Return: sparce matrix with shape [hash_size, n_docs] ([hashed_ngram, docs])\n",
    "    \"\"\"\n",
    "    # Map doc_ids to indexes\n",
    "    global DOC2IDX\n",
    "    db_class = retriever.get_class(db)  # class DocDB\n",
    "    with db_class(**db_opts) as doc_db:\n",
    "        doc_ids = doc_db.get_doc_ids()  # get all doc ids, (a list)\n",
    "    DOC2IDX = {doc_id: i for i, doc_id in enumerate(doc_ids)}  # {doc_ids: numeric index}\n",
    "\n",
    "    # Setup worker pool\n",
    "    tok_class = tokenizers.get_class(args.tokenizer)   # 4 tokenizer to choose\n",
    "    workers = ProcessPool(   # initialize Pool workers\n",
    "        args.num_workers,\n",
    "        initializer=init,\n",
    "        initargs=(tok_class, db_class, db_opts)\n",
    "    )\n",
    "\n",
    "    # Compute the count matrix in steps (to keep in memory)\n",
    "    logger.info('Mapping...')\n",
    "    row, col, data = [], [], []\n",
    "    step = max(int(len(doc_ids) / 10), 1)   # 10 is number_of_batchs, step is batch_size\n",
    "    batches = [doc_ids[i:i + step] for i in range(0, len(doc_ids), step)]  # create batches\n",
    "    _count = partial(count, args.ngram, args.hash_size)  # count function with bigram and 2**24 bins\n",
    "    for i, batch in enumerate(batches):\n",
    "        logger.info('-' * 25 + 'Batch %d/%d' % (i + 1, len(batches)) + '-' * 25)\n",
    "        for b_row, b_col, b_data in workers.imap_unordered(_count, batch):   # count function apply to docs\n",
    "            row.extend(b_row)\n",
    "            col.extend(b_col)\n",
    "            data.extend(b_data)\n",
    "    workers.close()\n",
    "    workers.join()\n",
    "\n",
    "    logger.info('Creating sparse matrix...')\n",
    "    count_matrix = sp.csr_matrix(\n",
    "        (data, (row, col)), shape=(args.hash_size, len(doc_ids))  # sparse_matrix [hashed_ngram, docs]\n",
    "    )\n",
    "    count_matrix.sum_duplicates()\n",
    "    return count_matrix, (DOC2IDX, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix, doc_dict = get_count_matrix(\n",
    "        args, 'sqlite', {'db_path': args.db_path}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalize: what to do when exiting process:\n",
    "https://stackoverflow.com/questions/24717468/context-managers-and-multiprocessing-pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_matrix(cnts):\n",
    "    \"\"\"Convert the word count matrix into tfidf one.\n",
    "\n",
    "    tfidf = log(tf + 1) * log((N - Nt + 0.5) / (Nt + 0.5))\n",
    "    * tf = term frequency in document\n",
    "    * N = number of documents\n",
    "    * Nt = number of occurences of term in all documents\n",
    "    \"\"\"\n",
    "    Ns = get_doc_freqs(cnts)  # Nt, one dimension array with shape [hash_size]\n",
    "    idfs = np.log((cnts.shape[1] - Ns + 0.5) / (Ns + 0.5))  # [hash_size]\n",
    "    idfs[idfs < 0] = 0\n",
    "    idfs = sp.diags(idfs, 0)   # inverse document frequency with shape [hash_size, hash_size]\n",
    "    tfs = cnts.log1p()   # term frequency with shape [hash_size, n_docs]\n",
    "    tfidfs = idfs.dot(tfs)  # dot product: [hash_size, n_docs]\n",
    "    return tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:51:09.914258Z",
     "start_time": "2019-03-11T22:51:09.909166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 2., 0.],\n",
       "       [0., 0., 3.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "sp.diags([1,2,3], 0).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:03:43.922971Z",
     "start_time": "2019-03-11T23:03:43.913821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.37417581, 0.37549935, 0.20378499, 0.5638298 , 0.400142  ],\n",
       "       [0.39012129, 0.80376494, 0.22833869, 0.91804787, 0.37156071],\n",
       "       [1.66346211, 1.17767683, 2.08005168, 0.93570479, 1.85065382]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs = sp.diags([1,2,3], 0); print(idfs.shape)\n",
    "tfs = np.random.rand(3,5); print(tfs.shape)\n",
    "idfs.dot(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:06:47.208650Z",
     "start_time": "2019-03-11T23:06:47.204061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37417581, 0.37549935, 0.20378499, 0.5638298 , 0.400142  ],\n",
       "       [0.39012129, 0.80376494, 0.22833869, 0.91804787, 0.37156071],\n",
       "       [1.66346211, 1.17767683, 2.08005168, 0.93570479, 1.85065382]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this can actually be achieved by element-wise multiplication\n",
    "idfs = np.array([1,2,3])\n",
    "tfs * idfs.reshape(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:04:03.651446Z",
     "start_time": "2019-03-11T23:04:03.646805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 2., 0.],\n",
       "       [0., 0., 3.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:04:06.475065Z",
     "start_time": "2019-03-11T23:04:06.470968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37417581, 0.37549935, 0.20378499, 0.5638298 , 0.400142  ],\n",
       "       [0.19506065, 0.40188247, 0.11416934, 0.45902393, 0.18578035],\n",
       "       [0.55448737, 0.39255894, 0.69335056, 0.3119016 , 0.61688461]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_freqs(cnts):\n",
    "    \"\"\"Return word --> # of docs it appears in.\"\"\"\n",
    "    binary = (cnts > 0).astype(int)            # presence\n",
    "    freqs = np.array(binary.sum(1)).squeeze()  # number of occurences of term in all documents\n",
    "    return freqs   # one dimension array [hash_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:52:33.628020Z",
     "start_time": "2019-03-11T22:52:33.623615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements (1 diagonals) in DIAgonal format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = sp.diags([1,2,3], 0)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T22:53:14.129268Z",
     "start_time": "2019-03-11T22:53:14.124104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((matrix>0).astype(int).sum(1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename, matrix, metadata=None):\n",
    "    data = {\n",
    "        'data': matrix.data,\n",
    "        'indices': matrix.indices,\n",
    "        'indptr': matrix.indptr,\n",
    "        'shape': matrix.shape,\n",
    "        'metadata': metadata,\n",
    "    }\n",
    "    np.savez(filename, **data)\n",
    "\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    matrix = sp.csr_matrix((loader['data'], loader['indices'],\n",
    "                            loader['indptr']), shape=loader['shape'])\n",
    "    return matrix, loader['metadata'].item(0) if 'metadata' in loader else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting words...\n",
    "count_matrix, doc_dict = get_count_matrix(\n",
    "    args, 'sqlite', {'db_path': args.db_path}\n",
    ")\n",
    "\n",
    "# 'Making tfidf vectors...'\n",
    "tfidf = get_tfidf_matrix(count_matrix)\n",
    "\n",
    "# 'Getting word-doc frequencies...'\n",
    "freqs = get_doc_freqs(count_matrix)\n",
    "\n",
    "basename = os.path.splitext(os.path.basename(args.db_path))[0]\n",
    "basename += ('-tfidf-ngram=%d-hash=%d-tokenizer=%s' %\n",
    "             (args.ngram, args.hash_size, args.tokenizer))\n",
    "filename = os.path.join(args.out_dir, basename)\n",
    "\n",
    "# 'Saving to %s.npz' % filename\n",
    "metadata = {\n",
    "    'doc_freqs': freqs,\n",
    "    'tokenizer': args.tokenizer,\n",
    "    'hash_size': args.hash_size,\n",
    "    'ngram': args.ngram,\n",
    "    'doc_dict': doc_dict\n",
    "}\n",
    "retriever.utils.save_sparse_csr(filename, tfidf, metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
