{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:15:14.300920Z",
     "start_time": "2019-03-11T23:15:12.959015Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "\n",
    "from multiprocessing import Pool as ProcessPool\n",
    "from multiprocessing.util import Finalize\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "\n",
    "from drqa import retriever\n",
    "from drqa import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:15:17.776829Z",
     "start_time": "2019-03-11T23:15:17.772000Z"
    }
   },
   "outputs": [],
   "source": [
    "DOC2IDX = None\n",
    "PROCESS_TOK = None\n",
    "PROCESS_DB = None\n",
    "\n",
    "def init(tokenizer_class, db_class, db_opts):\n",
    "    global PROCESS_TOK, PROCESS_DB\n",
    "    PROCESS_TOK = tokenizer_class()\n",
    "    Finalize(PROCESS_TOK, PROCESS_TOK.shutdown, exitpriority=100)\n",
    "    PROCESS_DB = db_class(**db_opts)\n",
    "    Finalize(PROCESS_DB, PROCESS_DB.close, exitpriority=100)\n",
    "\n",
    "\n",
    "def fetch_text(doc_id):\n",
    "    global PROCESS_DB\n",
    "    return PROCESS_DB.get_doc_text(doc_id)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    global PROCESS_TOK\n",
    "    return PROCESS_TOK.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:15:47.444501Z",
     "start_time": "2019-03-11T23:15:47.440593Z"
    }
   },
   "outputs": [],
   "source": [
    "db_class = retriever.get_class('sqlite')\n",
    "doc_db = db_class(\"/Users/zhengjiexu/Desktop/DrQA/data/jay_test/test_docs.db\")\n",
    "doc_ids = doc_db.get_doc_ids()  \n",
    "DOC2IDX = {doc_id: i for i, doc_id in enumerate(doc_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:15:51.914682Z",
     "start_time": "2019-03-11T23:15:51.905733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'car': 0, 'cat': 1, 'dog': 2, 'robot': 3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOC2IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:17:21.392611Z",
     "start_time": "2019-03-11T23:17:19.185638Z"
    }
   },
   "outputs": [],
   "source": [
    "tok_class = tokenizers.get_class('spacy') \n",
    "PROCESS_TOK = tok_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:17:25.657768Z",
     "start_time": "2019-03-11T23:17:25.655142Z"
    }
   },
   "outputs": [],
   "source": [
    "PROCESS_DB = doc_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:17:37.114395Z",
     "start_time": "2019-03-11T23:17:37.073649Z"
    }
   },
   "outputs": [],
   "source": [
    "def count(ngram, hash_size, doc_id):\n",
    "    \"\"\"Fetch the text of a document and compute hashed ngrams counts.\"\"\"\n",
    "    global DOC2IDX\n",
    "    row, col, data = [], [], []\n",
    "    # Tokenize\n",
    "    tokens = tokenize(retriever.utils.normalize(fetch_text(doc_id)))\n",
    "\n",
    "    # Get ngrams from tokens, with stopword/punctuation filtering.\n",
    "    ngrams = tokens.ngrams(\n",
    "        n=ngram, uncased=True, filter_fn=retriever.utils.filter_ngram\n",
    "    )\n",
    "    print('----ngrams----')\n",
    "    print(ngrams[:10])\n",
    "    print('\\n----number of grams before hash----')\n",
    "    print(len(ngrams))\n",
    "\n",
    "    # Hash ngrams and count occurences\n",
    "    counts = Counter([retriever.utils.hash(gram, hash_size) for gram in ngrams])\n",
    "    print('\\n----number of grams after hash----')\n",
    "    print(len(counts))\n",
    "    \n",
    "\n",
    "    # Return in sparse matrix data format.\n",
    "    row.extend(counts.keys())\n",
    "    col.extend([DOC2IDX[doc_id]] * len(counts))\n",
    "    data.extend(counts.values())\n",
    "    return row, col, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:17:57.535577Z",
     "start_time": "2019-03-11T23:17:57.526487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ngrams----\n",
      "['robot', 'machine', 'especially', 'especially one', 'one', 'one programmable', 'programmable', 'computer—', 'computer— capable', 'capable']\n",
      "\n",
      "----number of grams before hash----\n",
      "17\n",
      "\n",
      "----number of grams after hash----\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "row, col, data = count(ngram=2, hash_size=2**24, doc_id='robot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:18:47.415173Z",
     "start_time": "2019-03-11T23:18:47.409796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2599104, 10120610, 16177419, 5477921, 9537228, 4992930, 14898010, 14564878, 14259886, 3938108, 5633020, 11336432, 2498602, 4731736, 8237383, 14811187, 4120384]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(row), print(col), print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:21:44.487436Z",
     "start_time": "2019-03-11T23:21:44.478076Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_count_matrix(args, db, db_opts):\n",
    "    \"\"\"Form a sparse word to document count matrix (inverted index).\n",
    "\n",
    "    M[i, j] = # times word i appears in document j.\n",
    "    \"\"\"\n",
    "    # Map doc_ids to indexes\n",
    "    global DOC2IDX\n",
    "    db_class = retriever.get_class(db)\n",
    "    with db_class(**db_opts) as doc_db:\n",
    "        doc_ids = doc_db.get_doc_ids()\n",
    "    DOC2IDX = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
    "\n",
    "    # Setup worker pool\n",
    "    tok_class = tokenizers.get_class(args.tokenizer)\n",
    "    workers = ProcessPool(\n",
    "        args.num_workers,\n",
    "        initializer=init,\n",
    "        initargs=(tok_class, db_class, db_opts)\n",
    "    )\n",
    "\n",
    "    # Compute the count matrix in steps (to keep in memory)\n",
    "    logger.info('Mapping...')\n",
    "    row, col, data = [], [], []\n",
    "    step = max(int(len(doc_ids) / 10), 1)\n",
    "    batches = [doc_ids[i:i + step] for i in range(0, len(doc_ids), step)]\n",
    "    _count = partial(count, args.ngram, args.hash_size)\n",
    "    for i, batch in enumerate(batches):\n",
    "        logger.info('-' * 25 + 'Batch %d/%d' % (i + 1, len(batches)) + '-' * 25)\n",
    "        for b_row, b_col, b_data in workers.imap_unordered(_count, batch):\n",
    "            row.extend(b_row)\n",
    "            col.extend(b_col)\n",
    "            data.extend(b_data)\n",
    "    workers.close()\n",
    "    workers.join()\n",
    "\n",
    "    logger.info('Creating sparse matrix...')\n",
    "    count_matrix = sp.csr_matrix(\n",
    "        (data, (row, col)), shape=(args.hash_size, len(doc_ids))\n",
    "    )\n",
    "    count_matrix.sum_duplicates()\n",
    "    return count_matrix, (DOC2IDX, doc_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:28:49.495780Z",
     "start_time": "2019-03-11T23:28:49.491593Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "fmt = logging.Formatter('%(asctime)s: [ %(message)s ]', '%m/%d/%Y %I:%M:%S %p')\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(fmt)\n",
    "logger.addHandler(console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:27:44.895298Z",
     "start_time": "2019-03-11T23:27:44.891807Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Args = namedtuple('Args', ['tokenizer', 'num_workers', 'ngram', 'hash_size', 'db_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:28:03.773413Z",
     "start_time": "2019-03-11T23:28:03.770512Z"
    }
   },
   "outputs": [],
   "source": [
    "args = Args('spacy', 1, 2, 2**24, \"/Users/zhengjiexu/Desktop/DrQA/data/jay_test/test_docs.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:28:10.784047Z",
     "start_time": "2019-03-11T23:28:10.779602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('spacy',\n",
       " 1,\n",
       " 2,\n",
       " 16777216,\n",
       " '/Users/zhengjiexu/Desktop/DrQA/data/jay_test/test_docs.db')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.tokenizer, args.num_workers, args.ngram, args.hash_size, args.db_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:28:55.190043Z",
     "start_time": "2019-03-11T23:28:53.024539Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/11/2019 04:28:53 PM: [ Mapping... ]\n",
      "03/11/2019 04:28:53 PM: [ -------------------------Batch 1/4------------------------- ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ngrams----\n",
      "['car', 'automobile', 'wheeled', 'wheeled motor', 'motor', 'motor vehicle', 'vehicle', 'vehicle used', 'used', 'transportation']\n",
      "\n",
      "----number of grams before hash----\n",
      "10\n",
      "\n",
      "----number of grams after hash----\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/11/2019 04:28:55 PM: [ -------------------------Batch 2/4------------------------- ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ngrams----\n",
      "['cat', 'felis', 'felis catus', 'catus', 'small', 'small carnivorous', 'carnivorous', 'carnivorous mammal.[1][2', 'mammal.[1][2', 'domesticated']\n",
      "\n",
      "----number of grams before hash----\n",
      "26\n",
      "\n",
      "----number of grams after hash----\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/11/2019 04:28:55 PM: [ -------------------------Batch 3/4------------------------- ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ngrams----\n",
      "['domestic', 'domestic dog', 'dog', 'canis', 'canis lupus', 'lupus', 'lupus familiaris', 'familiaris', 'considered', 'subspecies']\n",
      "\n",
      "----number of grams before hash----\n",
      "37\n",
      "\n",
      "----number of grams after hash----\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/11/2019 04:28:55 PM: [ -------------------------Batch 4/4------------------------- ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ngrams----\n",
      "['robot', 'machine', 'especially', 'especially one', 'one', 'one programmable', 'programmable', 'computer—', 'computer— capable', 'capable']\n",
      "\n",
      "----number of grams before hash----\n",
      "17\n",
      "\n",
      "----number of grams after hash----\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/11/2019 04:28:55 PM: [ Creating sparse matrix... ]\n"
     ]
    }
   ],
   "source": [
    "count_matrix, doc_dict = get_count_matrix(\n",
    "        args, 'sqlite', {'db_path': args.db_path}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:30:48.164063Z",
     "start_time": "2019-03-11T23:30:48.159886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16777216, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:30:56.837571Z",
     "start_time": "2019-03-11T23:30:56.833504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'car': 0, 'cat': 1, 'dog': 2, 'robot': 3}, ['car', 'cat', 'dog', 'robot'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:34:15.270128Z",
     "start_time": "2019-03-11T23:34:15.264951Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tfidf_matrix(cnts):\n",
    "    \"\"\"Convert the word count matrix into tfidf one.\n",
    "\n",
    "    tfidf = log(tf + 1) * log((N - Nt + 0.5) / (Nt + 0.5))\n",
    "    * tf = term frequency in document\n",
    "    * N = number of documents\n",
    "    * Nt = number of occurences of term in all documents\n",
    "    \"\"\"\n",
    "    print('--count_matrix shape--:', count_matrix.shape)\n",
    "    Ns = get_doc_freqs(cnts); print('--Ns shape--:', Ns.shape)\n",
    "    idfs = np.log((cnts.shape[1] - Ns + 0.5) / (Ns + 0.5)); print('--idfs shape--:', idfs.shape)\n",
    "    idfs[idfs < 0] = 0\n",
    "    idfs = sp.diags(idfs, 0); print('--diag shape--:', idfs.shape)\n",
    "    tfs = cnts.log1p(); print('--tfs shape--:', tfs.shape)\n",
    "    tfidfs = idfs.dot(tfs); print('--tfidfs--:', tfidfs.shape)\n",
    "    return tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:34:16.224105Z",
     "start_time": "2019-03-11T23:34:16.220476Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_freqs(cnts):\n",
    "    \"\"\"Return word --> # of docs it appears in.\"\"\"\n",
    "    binary = (cnts > 0).astype(int)\n",
    "    freqs = np.array(binary.sum(1)).squeeze()\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:34:18.916551Z",
     "start_time": "2019-03-11T23:34:16.354166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--count_matrix shape--: (16777216, 4)\n",
      "--Ns shape--: (16777216,)\n",
      "--idfs shape--: (16777216,)\n",
      "--diag shape--: (16777216, 16777216)\n",
      "--tfs shape--: (16777216, 4)\n",
      "--tfidfs--: (16777216, 4)\n"
     ]
    }
   ],
   "source": [
    "tfidf = get_tfidf_matrix(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:35:04.635731Z",
     "start_time": "2019-03-11T23:35:04.631903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16777216, 4)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:34:51.755321Z",
     "start_time": "2019-03-11T23:34:51.356316Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_freqs = get_doc_freqs(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T23:34:58.886454Z",
     "start_time": "2019-03-11T23:34:58.882346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16777216,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_freqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
