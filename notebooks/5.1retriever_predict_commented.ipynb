{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:08:51.569248Z",
     "start_time": "2019-03-12T22:08:51.566217Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.model is the saved tfidf_matrix.npy\n",
    "# ranker is TfidfDocRanker object\n",
    "ranker = retriever.get_class('tfidf')(tfidf_path=args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(query, k=1):\n",
    "    doc_names, doc_scores = ranker.closest_docs(query, k)\n",
    "    table = prettytable.PrettyTable(\n",
    "        ['Rank', 'Doc Id', 'Doc Score']\n",
    "    )\n",
    "    for i in range(len(doc_names)):\n",
    "        table.add_row([i + 1, doc_names[i], '%.5g' % doc_scores[i]])\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     24,
     28
    ]
   },
   "outputs": [],
   "source": [
    "class TfidfDocRanker(object):\n",
    "    \"\"\"Loads a pre-weighted inverted index of token/document terms.\n",
    "    Scores new queries by taking sparse dot products.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tfidf_path=None, strict=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tfidf_path: path to saved model file\n",
    "            strict: fail on empty queries or continue (and return empty result)\n",
    "        \"\"\"\n",
    "        # Load from disk\n",
    "        tfidf_path = tfidf_path or DEFAULTS['tfidf_path']\n",
    "        logger.info('Loading %s' % tfidf_path)\n",
    "        matrix, metadata = utils.load_sparse_csr(tfidf_path)\n",
    "        self.doc_mat = matrix\n",
    "        self.ngrams = metadata['ngram']\n",
    "        self.hash_size = metadata['hash_size']\n",
    "        self.tokenizer = tokenizers.get_class(metadata['tokenizer'])()\n",
    "        self.doc_freqs = metadata['doc_freqs'].squeeze()\n",
    "        self.doc_dict = metadata['doc_dict']\n",
    "        self.num_docs = len(self.doc_dict[0])\n",
    "        self.strict = strict\n",
    "\n",
    "    def get_doc_index(self, doc_id):\n",
    "        \"\"\"Convert doc_id --> doc_index\"\"\"\n",
    "        return self.doc_dict[0][doc_id]\n",
    "\n",
    "    def get_doc_id(self, doc_index):\n",
    "        \"\"\"Convert doc_index --> doc_id\"\"\"\n",
    "        # self.doc_dict: (DOC2IDX, doc_ids)\n",
    "        return self.doc_dict[1][doc_index]\n",
    "\n",
    "    def closest_docs(self, query, k=1):\n",
    "        \"\"\"Closest docs by dot product between query and documents\n",
    "        in tfidf weighted word vector space.\n",
    "        \"\"\"\n",
    "        spvec = self.text2spvec(query) # shape: [1, hash_size]\n",
    "        \n",
    "        # A common operation on sparse matrices is to multiply them by a dense vector.\n",
    "        # In such an operation, the result is the dot-product of each sparse row of the matrix with the dense vector.\n",
    "        # res.data is scores, res.indices is doc_index\n",
    "        res = spvec * self.doc_mat  # dot-product; spvec: [1, hash_size]; tfidf_matrix: [hash_size, num_docs]; res [1, num_docs]\n",
    "\n",
    "        # res.data are nonzero values\n",
    "        if len(res.data) <= k:\n",
    "            o_sort = np.argsort(-res.data)\n",
    "        else:\n",
    "            # use np.argpartition to first partition/select top k elements\n",
    "            # then sort the k elements\n",
    "            o = np.argpartition(-res.data, k)[0:k]\n",
    "            o_sort = o[np.argsort(-res.data[o])]  # index of the top k elements, sorted\n",
    "        \n",
    "        # res.data is scores, res.indices is doc_index\n",
    "        doc_scores = res.data[o_sort]\n",
    "        doc_ids = [self.get_doc_id(i) for i in res.indices[o_sort]]\n",
    "        return doc_ids, doc_scores\n",
    "\n",
    "    def batch_closest_docs(self, queries, k=1, num_workers=None):\n",
    "        \"\"\"Process a batch of closest_docs requests multithreaded.\n",
    "        Note: we can use plain threads here as scipy is outside of the GIL.\n",
    "        \"\"\"\n",
    "        with ThreadPool(num_workers) as threads:\n",
    "            closest_docs = partial(self.closest_docs, k=k)\n",
    "            results = threads.map(closest_docs, queries)\n",
    "        return results\n",
    "\n",
    "    def parse(self, query):\n",
    "        \"\"\"Parse the query into tokens (either ngrams or tokens).\"\"\"\n",
    "        tokens = self.tokenizer.tokenize(query)\n",
    "        return tokens.ngrams(n=self.ngrams, uncased=True,\n",
    "                             filter_fn=utils.filter_ngram)  # filter_ngram: remove stopword and punctuation\n",
    "\n",
    "    def text2spvec(self, query):\n",
    "        \"\"\"Create a sparse tfidf-weighted word vector from query.\n",
    "\n",
    "        tfidf = log(tf + 1) * log((N - Nt + 0.5) / (Nt + 0.5))\n",
    "        \"\"\"\n",
    "        # Get hashed ngrams\n",
    "        words = self.parse(utils.normalize(query))\n",
    "        wids = [utils.hash(w, self.hash_size) for w in words]\n",
    "\n",
    "        if len(wids) == 0:\n",
    "            if self.strict:\n",
    "                raise RuntimeError('No valid word in: %s' % query)\n",
    "            else:\n",
    "                logger.warning('No valid word in: %s' % query)\n",
    "                return sp.csr_matrix((1, self.hash_size))\n",
    "\n",
    "        # Count TF\n",
    "        wids_unique, wids_counts = np.unique(wids, return_counts=True)\n",
    "        tfs = np.log1p(wids_counts)\n",
    "\n",
    "        # Count IDF\n",
    "        Ns = self.doc_freqs[wids_unique]\n",
    "        idfs = np.log((self.num_docs - Ns + 0.5) / (Ns + 0.5))\n",
    "        idfs[idfs < 0] = 0\n",
    "\n",
    "        # TF-IDF\n",
    "        data = np.multiply(tfs, idfs)  # shape: [1, num_unique_grams]\n",
    "\n",
    "        # One row, sparse csr matrix\n",
    "        # return a tfidf vector\n",
    "        indptr = np.array([0, len(wids_unique)])\n",
    "        spvec = sp.csr_matrix(\n",
    "            (data, wids_unique, indptr), shape=(1, self.hash_size)  # shape: [1, hash_size]\n",
    "        )\n",
    "\n",
    "        return spvec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**exploring `text2spvec`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:55:29.492107Z",
     "start_time": "2019-03-12T21:55:29.488323Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([1,2,3,4])\n",
    "wids_unique = np.array([8, 12, 6, 9])\n",
    "indptr = np.array([0, len(wids_unique)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:56:23.627951Z",
     "start_time": "2019-03-12T21:56:23.624457Z"
    }
   },
   "outputs": [],
   "source": [
    "hash_size = 20\n",
    "# csr_matrix((data, indices, indptr)\n",
    "spvec = sp.csr_matrix((data, wids_unique, indptr), \n",
    "                      shape=(1, hash_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:56:27.059682Z",
     "start_time": "2019-03-12T21:56:27.055522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:56:34.699377Z",
     "start_time": "2019-03-12T21:56:34.694865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 3, 0, 1, 4, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spvec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:58:22.747836Z",
     "start_time": "2019-03-12T21:58:22.743086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0], dtype=int32), array([ 8, 12,  6,  9], dtype=int32))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spvec.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:02:30.333393Z",
     "start_time": "2019-03-12T22:02:30.328703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spvec.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:02:44.552850Z",
     "start_time": "2019-03-12T22:02:44.548485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 12,  6,  9], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spvec.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:03:00.483545Z",
     "start_time": "2019-03-12T22:03:00.479373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spvec.indptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**exploring np.argpartition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:09:53.481237Z",
     "start_time": "2019-03-12T22:09:53.476850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  2,  6,  7,  3, 17, 11, 14, 16,  4])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = np.random.randint(1,20,10)\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:13:14.231699Z",
     "start_time": "2019-03-12T22:13:14.227238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 3, 4, 6, 7, 11, 14, 16, 17]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:10:11.706461Z",
     "start_time": "2019-03-12T22:10:11.701930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 2, 9, 3, 5, 7, 8, 6])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partition the numbers, so that numbers smaller than kth elements are put on left, largers are on the right\n",
    "# thus, we can select the top k ements by choosing the first K elements\n",
    "# note that those K elements are not sorted, we need to sort them\n",
    "np.argpartition(nums, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:12:55.634863Z",
     "start_time": "2019-03-12T22:12:55.630255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 6, 4])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take top K elements\n",
    "top5 = nums[np.argpartition(nums, 5)[:5]]\n",
    "top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:13:44.111483Z",
     "start_time": "2019-03-12T22:13:44.107476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 4, 6])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that those K elements are not sorted, we need to sort them\n",
    "top5[np.argsort(top5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explore ngram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(self, n=1, uncased=False, filter_fn=None, as_strings=True):\n",
    "    \"\"\"Returns a list of all ngrams from length 1 to n.\n",
    "\n",
    "    Args:\n",
    "        n: upper limit of ngram length\n",
    "        uncased: lower cases text\n",
    "        filter_fn: user function that takes in an ngram list and returns\n",
    "          True or False to keep or not keep the ngram\n",
    "        as_string: return the ngram as a string vs list\n",
    "    \"\"\"\n",
    "    def _skip(gram):\n",
    "        \"\"\"if _skip is True, it is stopwords or punctuation, skip the word\n",
    "        \"\"\"\n",
    "        if not filter_fn:\n",
    "            return False\n",
    "        return filter_fn(gram)\n",
    "\n",
    "    words = self.words(uncased)\n",
    "    ngrams = [(s, e + 1)\n",
    "              for s in range(len(words))\n",
    "              for e in range(s, min(s + n, len(words)))\n",
    "              if not _skip(words[s:e + 1])]  # if _skip is False, keep the word\n",
    "\n",
    "    # Concatenate into strings\n",
    "    if as_strings:\n",
    "        ngrams = ['{}'.format(' '.join(words[s:e])) for (s, e) in ngrams]\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T23:11:31.331004Z",
     "start_time": "2019-03-12T23:11:31.325202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'i have',\n",
       " 'have',\n",
       " 'have read',\n",
       " 'read',\n",
       " 'read the',\n",
       " 'the',\n",
       " 'the book',\n",
       " 'book',\n",
       " 'book already',\n",
       " 'already']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['i', 'have', 'read', 'the', 'book', 'already']\n",
    "n = 2\n",
    "ngrams = [(s, e + 1)\n",
    "          for s in range(len(words))\n",
    "          for e in range(s, min(s + n, len(words)))]\n",
    "grams = ['{}'.format(' '.join(words[s:e])) for (s, e) in ngrams]\n",
    "grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T23:09:44.570165Z",
     "start_time": "2019-03-12T23:09:44.566277Z"
    }
   },
   "outputs": [],
   "source": [
    "def ngrams_func(words, n):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i<len(words):\n",
    "        j = i\n",
    "        while j<i+n and j<len(words):\n",
    "            result.append([words[i:j+1]])\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T23:09:48.992490Z",
     "start_time": "2019-03-12T23:09:48.987820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['i']],\n",
       " [['i', 'have']],\n",
       " [['have']],\n",
       " [['have', 'read']],\n",
       " [['read']],\n",
       " [['read', 'the']],\n",
       " [['the']],\n",
       " [['the', 'book']],\n",
       " [['book']],\n",
       " [['book', 'already']],\n",
       " [['already']]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_func(words, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
