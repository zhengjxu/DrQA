{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "from deeppavlov.core.common.registry import register\n",
    "from deeppavlov.core.models.component import Component\n",
    "from deeppavlov.core.commands.utils import expand_path\n",
    "\n",
    "\n",
    "@register('use_sentence_ranker')\n",
    "class USESentenceRanker(Component):\n",
    "    def __init__(self, top_n=20, return_vectors=False, active: bool = True, **kwargs):\n",
    "        \"\"\"\n",
    "        :param top_n: top n sentences to return\n",
    "        :param return_vectors: return unranged USE vectors instead of sentences\n",
    "        :param active: when is not active, return all sentences\n",
    "        \"\"\"\n",
    "        self.embed = hub.Module(str(expand_path(\"general_electrics/hub\")))\n",
    "        self.session = tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=tf.GPUOptions(\n",
    "                per_process_gpu_memory_fraction=0.4,\n",
    "                allow_growth=False\n",
    "            )))\n",
    "        self.session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        self.q_ph = tf.placeholder(shape=(None,), dtype=tf.string)  # question placeholder\n",
    "        self.c_ph = tf.placeholder(shape=(None,), dtype=tf.string)  # context placeholder\n",
    "        self.q_emb = self.embed(self.q_ph)  # question embeddng\n",
    "        self.c_emb = self.embed(self.c_ph)  # context embedding\n",
    "        self.top_n = top_n\n",
    "        self.return_vectors = return_vectors\n",
    "        self.active = active\n",
    "\n",
    "    def __call__(self, query_context: List[Tuple[str, List[str]]]):\n",
    "        \"\"\"\n",
    "        Rank sentences and return top n sentences.\n",
    "        Args: \n",
    "            [(query1_str, [context1_str, context2_str...]), \n",
    "             (query2_str, [context...])]\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        all_top_scores = []\n",
    "        fake_scores = [0.001] * len(query_context)\n",
    "\n",
    "        for el in query_context:  # el means elements\n",
    "            # DEBUG\n",
    "            # start_time = time.time()\n",
    "            qe, ce = self.session.run([self.q_emb, self.c_emb],  # qe: query embedding; ce: context embeddings matrix\n",
    "                                      feed_dict={\n",
    "                                          self.q_ph: [el[0]], # 1 query string\n",
    "                                          self.c_ph: el[1],   # list of context strings\n",
    "                                      })\n",
    "            # print(\"Time spent: {}\".format(time.time() - start_time))\n",
    "            if self.return_vectors:\n",
    "                predictions.append((qe, ce))\n",
    "            else:\n",
    "                scores = (qe @ ce.T).squeeze()  # cosine similary(note that USE embedding are approximately normalized, so we do not normalize them)\n",
    "                if self.active:  # return top sents\n",
    "                    thresh = self.top_n\n",
    "                else:            # return all sents\n",
    "                    thresh = len(el[1])\n",
    "                if scores.size == 1:   # when only has 1 context string\n",
    "                    scores = [scores]\n",
    "                top_scores = np.sort(scores)[::-1][:thresh]\n",
    "                all_top_scores.append(top_scores)\n",
    "                # Sorts the sentences!\n",
    "                # sentence_top_ids = np.argsort(scores)[::-1][:thresh]\n",
    "                # predictions.append([el[1][x] for x in sentence_top_ids])\n",
    "                predictions.append(el[1])\n",
    "        if self.return_vectors:\n",
    "            return predictions, fake_scores\n",
    "        else:\n",
    "            return [' '.join(sentences) for sentences in predictions], all_top_scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
